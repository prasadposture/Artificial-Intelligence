{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prasad Rajesh Posture  \n",
    "**Batch**: July 2022  \n",
    "Artificial Intelligence  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task :** Create a deep learning model that generates meaningful text based on your input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-27T06:02:23.363592Z",
     "iopub.status.busy": "2022-07-27T06:02:23.363177Z",
     "iopub.status.idle": "2022-07-27T06:02:29.561044Z",
     "shell.execute_reply": "2022-07-27T06:02:29.560054Z",
     "shell.execute_reply.started": "2022-07-27T06:02:23.363508Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Import Dependencies\n",
    "import numpy\n",
    "import sys\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-27T06:02:29.563303Z",
     "iopub.status.busy": "2022-07-27T06:02:29.562757Z",
     "iopub.status.idle": "2022-07-27T06:02:29.578221Z",
     "shell.execute_reply": "2022-07-27T06:02:29.577390Z",
     "shell.execute_reply.started": "2022-07-27T06:02:29.563276Z"
    }
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "# loading data and opening our input data in the form of a txt file\n",
    "# Project Gutenburg/berg is where the data can be found\n",
    "file=open(\"../input/frankenstein/frankenstein.txt\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-27T06:02:29.581554Z",
     "iopub.status.busy": "2022-07-27T06:02:29.581286Z",
     "iopub.status.idle": "2022-07-27T06:02:38.511303Z",
     "shell.execute_reply": "2022-07-27T06:02:38.510379Z",
     "shell.execute_reply.started": "2022-07-27T06:02:29.581530Z"
    }
   },
   "outputs": [],
   "source": [
    "# tokenization\n",
    "# standardization\n",
    "#what is tokenization? Tokenization is the process of breaking a stream of text up into words phrases symbols or some meaningful elements\n",
    "def tokenize_words(input):\n",
    "    input=input.lower()\n",
    "    tokenizer=RegexpTokenizer(r'\\w+')\n",
    "    tokens=tokenizer.tokenize(input)\n",
    "    filtered=filter(lambda token: token not in stopwords.words('english'), tokens)\n",
    "    return \" \".join(filtered)\n",
    "processed_inputs=tokenize_words(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-27T06:02:38.514464Z",
     "iopub.status.busy": "2022-07-27T06:02:38.514057Z",
     "iopub.status.idle": "2022-07-27T06:02:38.524984Z",
     "shell.execute_reply": "2022-07-27T06:02:38.521813Z",
     "shell.execute_reply.started": "2022-07-27T06:02:38.514421Z"
    }
   },
   "outputs": [],
   "source": [
    "#chara to numbers\n",
    "chars=sorted(list(set(processed_inputs)))\n",
    "char_to_num=dict((c,i) for i,c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-27T06:02:38.526841Z",
     "iopub.status.busy": "2022-07-27T06:02:38.526480Z",
     "iopub.status.idle": "2022-07-27T06:02:38.541569Z",
     "shell.execute_reply": "2022-07-27T06:02:38.540324Z",
     "shell.execute_reply.started": "2022-07-27T06:02:38.526805Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters: 269566\n",
      "Total vocab: 38\n"
     ]
    }
   ],
   "source": [
    "#check if the words to char or chars to num has worked\n",
    "input_len=len(processed_inputs)\n",
    "vocab_len=len(chars)\n",
    "print(\"Total number of characters:\", input_len)\n",
    "print(\"Total vocab:\",vocab_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-27T06:02:38.543638Z",
     "iopub.status.busy": "2022-07-27T06:02:38.543007Z",
     "iopub.status.idle": "2022-07-27T06:02:38.551490Z",
     "shell.execute_reply": "2022-07-27T06:02:38.550669Z",
     "shell.execute_reply.started": "2022-07-27T06:02:38.543601Z"
    }
   },
   "outputs": [],
   "source": [
    "#sege length\n",
    "seq_length=100\n",
    "x_data=[]\n",
    "y_data=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-27T06:02:38.553470Z",
     "iopub.status.busy": "2022-07-27T06:02:38.553039Z",
     "iopub.status.idle": "2022-07-27T06:02:41.034875Z",
     "shell.execute_reply": "2022-07-27T06:02:41.033865Z",
     "shell.execute_reply.started": "2022-07-27T06:02:38.553346Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns: 269466\n"
     ]
    }
   ],
   "source": [
    "#loop through the sequence\n",
    "for i in range(0,input_len-seq_length,1):\n",
    "    in_seq=processed_inputs[i:i+seq_length]\n",
    "    out_seq=processed_inputs[i+seq_length]\n",
    "    x_data.append([char_to_num[char] for char in in_seq])\n",
    "    y_data.append(char_to_num[out_seq])\n",
    "    \n",
    "n_patterns=len(x_data)\n",
    "print(\"Total Patterns:\", n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-27T06:02:41.036863Z",
     "iopub.status.busy": "2022-07-27T06:02:41.036491Z",
     "iopub.status.idle": "2022-07-27T06:02:44.435668Z",
     "shell.execute_reply": "2022-07-27T06:02:44.434586Z",
     "shell.execute_reply.started": "2022-07-27T06:02:41.036826Z"
    }
   },
   "outputs": [],
   "source": [
    "# convert input sequence to np array and so on\n",
    "X=numpy.reshape(x_data,(n_patterns, seq_length,1))\n",
    "X=X/float(vocab_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-27T06:02:44.438703Z",
     "iopub.status.busy": "2022-07-27T06:02:44.436970Z",
     "iopub.status.idle": "2022-07-27T06:02:44.489309Z",
     "shell.execute_reply": "2022-07-27T06:02:44.488407Z",
     "shell.execute_reply.started": "2022-07-27T06:02:44.438660Z"
    }
   },
   "outputs": [],
   "source": [
    "# One-hot encoding\n",
    "y=np_utils.to_categorical(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-27T06:02:44.494647Z",
     "iopub.status.busy": "2022-07-27T06:02:44.493819Z",
     "iopub.status.idle": "2022-07-27T06:02:48.066954Z",
     "shell.execute_reply": "2022-07-27T06:02:48.066010Z",
     "shell.execute_reply.started": "2022-07-27T06:02:44.494606Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-27 06:02:44.576114: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-27 06:02:44.697261: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-27 06:02:44.698100: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-27 06:02:44.699827: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-07-27 06:02:44.700142: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-27 06:02:44.700866: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-27 06:02:44.701530: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-27 06:02:46.870666: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-27 06:02:46.872086: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-27 06:02:46.873252: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-27 06:02:46.874483: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15403 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n"
     ]
    }
   ],
   "source": [
    "#Creating the model\n",
    "model=Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1],X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1],activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-27T06:02:48.068677Z",
     "iopub.status.busy": "2022-07-27T06:02:48.068321Z",
     "iopub.status.idle": "2022-07-27T06:02:48.088706Z",
     "shell.execute_reply": "2022-07-27T06:02:48.087873Z",
     "shell.execute_reply.started": "2022-07-27T06:02:48.068643Z"
    }
   },
   "outputs": [],
   "source": [
    "#Compile the model\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-27T06:02:48.090328Z",
     "iopub.status.busy": "2022-07-27T06:02:48.089906Z",
     "iopub.status.idle": "2022-07-27T06:02:48.095793Z",
     "shell.execute_reply": "2022-07-27T06:02:48.094641Z",
     "shell.execute_reply.started": "2022-07-27T06:02:48.090291Z"
    }
   },
   "outputs": [],
   "source": [
    "#saving the weights\n",
    "filepath='model_weights_saved.hdf5'\n",
    "checkpoint=ModelCheckpoint(filepath, monitor='loss',verbose=1,save_best_only=True,mode='min')\n",
    "desired_callbacks=[checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-27T06:02:48.098240Z",
     "iopub.status.busy": "2022-07-27T06:02:48.097551Z",
     "iopub.status.idle": "2022-07-27T06:14:13.933344Z",
     "shell.execute_reply": "2022-07-27T06:14:13.932383Z",
     "shell.execute_reply.started": "2022-07-27T06:02:48.098205Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-27 06:02:48.444967: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-27 06:02:52.827949: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1053/1053 [==============================] - 72s 63ms/step - loss: 2.7874\n",
      "\n",
      "Epoch 00001: loss improved from inf to 2.78743, saving model to model_weights_saved.hdf5\n",
      "Epoch 2/10\n",
      "1053/1053 [==============================] - 66s 62ms/step - loss: 2.4565\n",
      "\n",
      "Epoch 00002: loss improved from 2.78743 to 2.45650, saving model to model_weights_saved.hdf5\n",
      "Epoch 3/10\n",
      "1053/1053 [==============================] - 66s 63ms/step - loss: 2.2023\n",
      "\n",
      "Epoch 00003: loss improved from 2.45650 to 2.20228, saving model to model_weights_saved.hdf5\n",
      "Epoch 4/10\n",
      "1053/1053 [==============================] - 66s 63ms/step - loss: 2.0206\n",
      "\n",
      "Epoch 00004: loss improved from 2.20228 to 2.02055, saving model to model_weights_saved.hdf5\n",
      "Epoch 5/10\n",
      "1053/1053 [==============================] - 66s 63ms/step - loss: 1.9013\n",
      "\n",
      "Epoch 00005: loss improved from 2.02055 to 1.90130, saving model to model_weights_saved.hdf5\n",
      "Epoch 6/10\n",
      "1053/1053 [==============================] - 66s 63ms/step - loss: 1.8219\n",
      "\n",
      "Epoch 00006: loss improved from 1.90130 to 1.82192, saving model to model_weights_saved.hdf5\n",
      "Epoch 7/10\n",
      "1053/1053 [==============================] - 66s 63ms/step - loss: 1.7616\n",
      "\n",
      "Epoch 00007: loss improved from 1.82192 to 1.76161, saving model to model_weights_saved.hdf5\n",
      "Epoch 8/10\n",
      "1053/1053 [==============================] - 66s 63ms/step - loss: 1.7127\n",
      "\n",
      "Epoch 00008: loss improved from 1.76161 to 1.71275, saving model to model_weights_saved.hdf5\n",
      "Epoch 9/10\n",
      "1053/1053 [==============================] - 66s 63ms/step - loss: 1.6782\n",
      "\n",
      "Epoch 00009: loss improved from 1.71275 to 1.67817, saving model to model_weights_saved.hdf5\n",
      "Epoch 10/10\n",
      "1053/1053 [==============================] - 66s 63ms/step - loss: 1.6465\n",
      "\n",
      "Epoch 00010: loss improved from 1.67817 to 1.64654, saving model to model_weights_saved.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5de37bb510>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit the model and let it train\n",
    "model.fit(X,y,epochs=10,batch_size=256,callbacks=desired_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-27T06:14:13.935332Z",
     "iopub.status.busy": "2022-07-27T06:14:13.934956Z",
     "iopub.status.idle": "2022-07-27T06:14:13.963732Z",
     "shell.execute_reply": "2022-07-27T06:14:13.962857Z",
     "shell.execute_reply.started": "2022-07-27T06:14:13.935295Z"
    }
   },
   "outputs": [],
   "source": [
    "#recompile the model with saved weights\n",
    "filename='model_weights_saved.hdf5'\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-27T06:14:13.965537Z",
     "iopub.status.busy": "2022-07-27T06:14:13.965168Z",
     "iopub.status.idle": "2022-07-27T06:14:13.972662Z",
     "shell.execute_reply": "2022-07-27T06:14:13.971778Z",
     "shell.execute_reply.started": "2022-07-27T06:14:13.965502Z"
    }
   },
   "outputs": [],
   "source": [
    "# output of the models back into characters\n",
    "num_to_char=dict((i,c) for i,c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-27T06:14:13.974444Z",
     "iopub.status.busy": "2022-07-27T06:14:13.973907Z",
     "iopub.status.idle": "2022-07-27T06:14:13.982569Z",
     "shell.execute_reply": "2022-07-27T06:14:13.981589Z",
     "shell.execute_reply.started": "2022-07-27T06:14:13.974408Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed :\n",
      "\" sh combined disdain malignity unearthly ugliness rendered almost horrible human eyes scarcely observ \"\n"
     ]
    }
   ],
   "source": [
    "#random seed to help generate\n",
    "start=numpy.random.randint(0,len(x_data)-1)\n",
    "pattern=x_data[start]\n",
    "print(\"Random Seed :\")\n",
    "print(\"\\\"\",''.join([num_to_char[value] for value in pattern]),\"\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-27T06:14:13.984943Z",
     "iopub.status.busy": "2022-07-27T06:14:13.984194Z",
     "iopub.status.idle": "2022-07-27T06:14:54.838477Z",
     "shell.execute_reply": "2022-07-27T06:14:54.837411Z",
     "shell.execute_reply.started": "2022-07-27T06:14:13.984903Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ed see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see see s"
     ]
    }
   ],
   "source": [
    "# generate the text\n",
    "for i in range(1000):\n",
    "    x=numpy.reshape(pattern,(1,len(pattern),1))\n",
    "    x=x/float(vocab_len)\n",
    "    prediction=model.predict(x, verbose=0)\n",
    "    index=numpy.argmax(prediction)\n",
    "    result=num_to_char[index]\n",
    "    seg_in=[num_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern=pattern[1:len(pattern)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
